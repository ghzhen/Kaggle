{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/\n",
    "\n",
    "Many machine learning models have either some inherent internal ranking of features or it is easy to generate the ranking from the structure of the model. This applies to regression models, SVM’s, decision trees, random forests, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multicollinearity \n",
    "\n",
    "https://en.wikipedia.org/wiki/Multicollinearity\n",
    "\n",
    "In statistics, multicollinearity (also collinearity) is a phenomenon in which two or more predictor variables in a multiple regression model are highly correlated, meaning that one can be linearly predicted from the others with a substantial degree of accuracy. In this situation the coefficient estimates of the multiple regression may change erratically in response to small changes in the model or the data. **Multicollinearity does not reduce the predictive power or reliability of the model as a whole**, at least within the sample data set; it only affects calculations regarding individual predictors. That is, a multiple regression model with correlated predictors can indicate how well the entire bundle of predictors predicts the outcome variable, **but it may not give valid results about any individual predictor, or about which predictors are redundant with respect to others**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "Regularization is a method for adding additional constraints or penalty to a model, with the goal of preventing overfitting and improving generalization. Instead of minimizing a loss function E(X,Y), the loss function to minimize becomes E(X,Y)+α‖w‖, where w is the vector of model coefficients, ‖⋅‖ is typically L1 or L2 norm and α is a tunable free parameter, specifying the amount of regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 regularization -- Lasso\n",
    "\n",
    " * α∑|wi|\n",
    " * L1 regularization produces sparse solutions, inherently performing feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso model: \n",
      "[(-3.71, 'LSTAT'), (-1.76, 'PTRATIO'), (-1.08, 'DIS'), (-0.7, 'NOX'), (-0.24, 'CRIM'), (-0.0, 'AGE'), (-0.0, 'INDUS'), (0.0, 'RAD'), (-0.0, 'TAX'), (0.08, 'ZN'), (0.54, 'CHAS'), (0.63, 'B'), (2.99, 'RM')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_boston\n",
    "  \n",
    "boston = load_boston()\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(boston[\"data\"])\n",
    "Y = boston[\"target\"]\n",
    "names = boston[\"feature_names\"]\n",
    "  \n",
    "lasso = Lasso(alpha=.3)\n",
    "lasso.fit(X, Y)\n",
    "\n",
    "print \"Lasso model: \"\n",
    "print sorted(zip([round(i,2) for i in lasso.coef_], names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to choose α（see last section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note however that L1 regularized regression is unstable (reason see example below) in a similar way as unregularized linear models are, meaning that the coefficients (and thus feature ranks) can vary significantly even on small data changes when there are correlated features in the data. Which brings us to L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 regularization -- Ridge regression\n",
    "\n",
    "* α∑wi^2\n",
    "* it has a different effect from L1-norm, namely it forces the coefficient values to be spread out more equally. For correlated features, it means that they tend to get similar coefficients. Coming back to the example of Y=X1+X2, with strongly correlated X1 and X2, then for L1, the penalty is the same whether the learned model is Y=1∗X1+1∗X2 or Y=2∗X1+0∗X2. In both cases the penalty is 2∗α. For L2 however, the first model’s penalty is 12+12=2α, while for the second model is penalized with 22+02=4α. **The effect of this is that models are much more stable**\n",
    "* So while L2 regularization **does not perform feature selection the same way as L1** does, it is more useful for ** feature *interpretation* **: a predictive feature will get a non-zero coefficient, which is often not the case with L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 0\n",
      "Linear model: [-0.08, 0.73, 2.31]\n",
      "Ridge model: [0.88, 0.94, 1.06]\n",
      "Random seed 1\n",
      "Linear model: [-0.6, 1.15, 2.37]\n",
      "Ridge model: [0.76, 0.98, 1.07]\n",
      "Random seed 2\n",
      "Linear model: [0.32, 0.7, 2.09]\n",
      "Ridge model: [0.94, 0.97, 1.09]\n",
      "Random seed 3\n",
      "Linear model: [0.29, 1.25, 1.49]\n",
      "Ridge model: [0.92, 1.0, 1.03]\n",
      "Random seed 4\n",
      "Linear model: [0.19, 0.77, 2.19]\n",
      "Ridge model: [0.96, 0.98, 1.1]\n",
      "Random seed 5\n",
      "Linear model: [-1.29, 1.59, 2.75]\n",
      "Ridge model: [0.76, 1.01, 1.14]\n",
      "Random seed 6\n",
      "Linear model: [-0.03, 1.2, 1.91]\n",
      "Ridge model: [0.89, 1.02, 1.09]\n",
      "Random seed 7\n",
      "Linear model: [-0.15, 1.47, 1.76]\n",
      "Ridge model: [0.9, 1.02, 1.04]\n",
      "Random seed 8\n",
      "Linear model: [0.08, 1.11, 1.88]\n",
      "Ridge model: [0.91, 1.01, 1.07]\n",
      "Random seed 9\n",
      "Linear model: [0.71, 0.78, 1.36]\n",
      "Ridge model: [0.9, 0.9, 0.98]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "size = 100\n",
    " \n",
    "#We run the method 10 times with different random seeds\n",
    "for i in range(10):\n",
    "    print \"Random seed %s\" % i\n",
    "    np.random.seed(seed=i)\n",
    "    X_seed = np.random.normal(0, 1, size)\n",
    "    X1 = X_seed + np.random.normal(0, .1, size)\n",
    "    X2 = X_seed + np.random.normal(0, .1, size)\n",
    "    X3 = X_seed + np.random.normal(0, .1, size)\n",
    "    Y = X1 + X2 + X3 + np.random.normal(0, 1, size)\n",
    "    X = np.array([X1, X2, X3]).T\n",
    " \n",
    " \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X,Y)\n",
    "    print \"Linear model:\", \n",
    "    print sorted([round(i,2) for i in lr.coef_])\n",
    "    #print sorted(zip([round(i,2) for i in lasso.coef_], names))\n",
    " \n",
    "    ridge = Ridge(alpha=10)\n",
    "    ridge.fit(X,Y)\n",
    "    print \"Ridge model:\",\n",
    "    print sorted([round(i,2) for i in ridge.coef_])\n",
    "    #print sorted(zip([round(i,2) for i in ridge.coef_], names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This session is about feature selection when features are correlated.\n",
    "\n",
    "Regularized linear models are a powerful set of tool for feature interpretation and selection. Lasso produces sparse solutions and as such is very useful selecting a strong subset of features for improving model performance. Ridge regression on the other hand can be used for data interpretation due to its stability and the fact that useful features tend to have non-zero coefficients. Since the relationship between the response variable and features in often non-linear, basis expansion can be used to convert features into a more suitable space, while keeping the simple linear models fully applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to select hyper parameter\n",
    "* http://scikit-learn.org/stable/modules/grid_search.html\n",
    "* using **Cross validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exhaustive Grid Search\n",
    "GridSearchCV will use the same shuffling for each set of parameters validated by a single call to its fit method\n",
    "\n",
    "example code : http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_digits.html#sphx-glr-auto-examples-model-selection-grid-search-digits-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'kernel': 'rbf', 'C': 10, 'gamma': 0.001}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.986 (+/-0.016) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.001}\n",
      "0.959 (+/-0.029) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.0001}\n",
      "0.988 (+/-0.017) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.001}\n",
      "0.982 (+/-0.026) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.0001}\n",
      "0.988 (+/-0.017) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.001}\n",
      "0.982 (+/-0.025) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.0001}\n",
      "0.988 (+/-0.017) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.001}\n",
      "0.982 (+/-0.025) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.0001}\n",
      "0.975 (+/-0.014) for {'kernel': 'linear', 'C': 1}\n",
      "0.975 (+/-0.014) for {'kernel': 'linear', 'C': 10}\n",
      "0.975 (+/-0.014) for {'kernel': 'linear', 'C': 100}\n",
      "0.975 (+/-0.014) for {'kernel': 'linear', 'C': 1000}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        89\n",
      "          1       0.97      1.00      0.98        90\n",
      "          2       0.99      0.98      0.98        92\n",
      "          3       1.00      0.99      0.99        93\n",
      "          4       1.00      1.00      1.00        76\n",
      "          5       0.99      0.98      0.99       108\n",
      "          6       0.99      1.00      0.99        89\n",
      "          7       0.99      1.00      0.99        78\n",
      "          8       1.00      0.98      0.99        92\n",
      "          9       0.99      0.99      0.99        92\n",
      "\n",
      "avg / total       0.99      0.99      0.99       899\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'kernel': 'rbf', 'C': 10, 'gamma': 0.001}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.986 (+/-0.019) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.001}\n",
      "0.957 (+/-0.029) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.0001}\n",
      "0.987 (+/-0.019) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.001}\n",
      "0.981 (+/-0.028) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.0001}\n",
      "0.987 (+/-0.019) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.001}\n",
      "0.981 (+/-0.026) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.0001}\n",
      "0.987 (+/-0.019) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.001}\n",
      "0.981 (+/-0.026) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.0001}\n",
      "0.972 (+/-0.012) for {'kernel': 'linear', 'C': 1}\n",
      "0.972 (+/-0.012) for {'kernel': 'linear', 'C': 10}\n",
      "0.972 (+/-0.012) for {'kernel': 'linear', 'C': 100}\n",
      "0.972 (+/-0.012) for {'kernel': 'linear', 'C': 1000}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        89\n",
      "          1       0.97      1.00      0.98        90\n",
      "          2       0.99      0.98      0.98        92\n",
      "          3       1.00      0.99      0.99        93\n",
      "          4       1.00      1.00      1.00        76\n",
      "          5       0.99      0.98      0.99       108\n",
      "          6       0.99      1.00      0.99        89\n",
      "          7       0.99      1.00      0.99        78\n",
      "          8       1.00      0.98      0.99        92\n",
      "          9       0.99      0.99      0.99        92\n",
      "\n",
      "avg / total       0.99      0.99      0.99       899\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Loading the Digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# To apply an classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "X = digits.images.reshape((n_samples, -1))\n",
    "y = digits.target\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Note the problem is too easy: the hyperparameter plateau is too flat and the\n",
    "# output model is the same for precision and recall with ties in quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example code 2: http://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html#sphx-glr-auto-examples-model-selection-plot-nested-cross-validation-iris-py\n",
    "\n",
    "I dont understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Average difference of 0.007742 with std. dev. of 0.007688.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "import numpy as np\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Number of random trials\n",
    "NUM_TRIALS = 30\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "# Set up possible values of parameters to optimize over\n",
    "p_grid = {\"C\": [1, 10, 100],\n",
    "          \"gamma\": [.01, .1]}\n",
    "\n",
    "# We will use a Support Vector Classifier with \"rbf\" kernel\n",
    "svr = SVC(kernel=\"rbf\")\n",
    "\n",
    "# Arrays to store scores\n",
    "non_nested_scores = np.zeros(NUM_TRIALS)\n",
    "nested_scores = np.zeros(NUM_TRIALS)\n",
    "\n",
    "# Loop for each trial\n",
    "for i in range(NUM_TRIALS):\n",
    "\n",
    "    # Choose cross-validation techniques for the inner and outer loops,\n",
    "    # independently of the dataset.\n",
    "    # E.g \"LabelKFold\", \"LeaveOneOut\", \"LeaveOneLabelOut\", etc.\n",
    "    inner_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n",
    "    outer_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n",
    "\n",
    "    # Non_nested parameter search and scoring\n",
    "    clf = GridSearchCV(estimator=svr, param_grid=p_grid, cv=inner_cv)\n",
    "    clf.fit(X_iris, y_iris)\n",
    "    non_nested_scores[i] = clf.best_score_\n",
    "\n",
    "    # Nested CV with parameter optimization\n",
    "    nested_score = cross_val_score(clf, X=X_iris, y=y_iris, cv=outer_cv)\n",
    "    nested_scores[i] = nested_score.mean()\n",
    "\n",
    "score_difference = non_nested_scores - nested_scores\n",
    "\n",
    "print(\"Average difference of {0:6f} with std. dev. of {1:6f}.\"\n",
    "      .format(score_difference.mean(), score_difference.std()))\n",
    "\n",
    "# Plot scores on each trial for nested and non-nested CV\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "non_nested_scores_line, = plt.plot(non_nested_scores, color='r')\n",
    "nested_line, = plt.plot(nested_scores, color='b')\n",
    "plt.ylabel(\"score\", fontsize=\"14\")\n",
    "plt.legend([non_nested_scores_line, nested_line],\n",
    "           [\"Non-Nested CV\", \"Nested CV\"],\n",
    "           bbox_to_anchor=(0, .4, .5, 0))\n",
    "plt.title(\"Non-Nested and Nested Cross Validation on Iris Dataset\",\n",
    "          x=.5, y=1.1, fontsize=\"15\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Parameter Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html#sphx-glr-auto-examples-model-selection-randomized-search-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 6.05 seconds for 20 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.923 (std: 0.001)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'min_samples_split': 4, 'criterion': 'entropy', 'max_features': 2, 'max_depth': None}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.917 (std: 0.006)\n",
      "Parameters: {'bootstrap': True, 'min_samples_leaf': 4, 'min_samples_split': 7, 'criterion': 'gini', 'max_features': 5, 'max_depth': None}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.913 (std: 0.010)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 8, 'min_samples_split': 6, 'criterion': 'entropy', 'max_features': 6, 'max_depth': None}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.913 (std: 0.008)\n",
      "Parameters: {'bootstrap': True, 'min_samples_leaf': 2, 'min_samples_split': 5, 'criterion': 'gini', 'max_features': 10, 'max_depth': None}\n",
      "\n",
      "GridSearchCV took 63.60 seconds for 216 candidate parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.934 (std: 0.011)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'min_samples_split': 3, 'criterion': 'gini', 'max_features': 3, 'max_depth': None}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.933 (std: 0.009)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'min_samples_split': 10, 'criterion': 'gini', 'max_features': 3, 'max_depth': None}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.932 (std: 0.014)\n",
      "Parameters: {'bootstrap': True, 'min_samples_leaf': 1, 'min_samples_split': 3, 'criterion': 'entropy', 'max_features': 10, 'max_depth': None}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.932 (std: 0.014)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 3, 'min_samples_split': 3, 'criterion': 'entropy', 'max_features': 10, 'max_depth': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# get some data\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# build a classifier\n",
    "clf = RandomForestClassifier(n_estimators=20)\n",
    "\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(2, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "start = time()\n",
    "random_search.fit(X, y)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.cv_results_)\n",
    "\n",
    "# use a full grid over all parameters\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid)\n",
    "start = time()\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.cv_results_['params'])))\n",
    "report(grid_search.cv_results_)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
