{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " univariate feature selection : each feature is evaluated independently with respect to the response variable\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view     ...      grade  sqft_above  \\\n",
       "0      5650     1.0           0     0     ...          7        1180   \n",
       "1      7242     2.0           0     0     ...          7        2170   \n",
       "2     10000     1.0           0     0     ...          6         770   \n",
       "3      5000     1.0           0     0     ...          7        1050   \n",
       "4      8080     1.0           0     0     ...          8        1680   \n",
       "\n",
       "   sqft_basement  yr_built  yr_renovated  zipcode      lat     long  \\\n",
       "0              0      1955             0    98178  47.5112 -122.257   \n",
       "1            400      1951          1991    98125  47.7210 -122.319   \n",
       "2              0      1933             0    98028  47.7379 -122.233   \n",
       "3            910      1965             0    98136  47.5208 -122.393   \n",
       "4              0      1987             0    98074  47.6168 -122.045   \n",
       "\n",
       "   sqft_living15  sqft_lot15  \n",
       "0           1340        5650  \n",
       "1           1690        7639  \n",
       "2           2720        8062  \n",
       "3           1360        5000  \n",
       "4           1800        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('kc_house_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df[['bathrooms','sqft_above','sqft_basement','sqft_lot','yr_built','yr_renovated','view','waterfront','zipcode']]\n",
    "y = df['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection: Univariate Selection\n",
    "* http://blog.datadive.net/selecting-good-features-part-i-univariate-selection/\n",
    "\n",
    "It can work for selecting top features for model improvement in some settings, but since it is **UNABLE TO REMOVE REDUNDANCY** (for example selecting only the best feature among **a subset of strongly correlated features**), this task is better left for other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson Correlation\n",
    "Scipy‘s pearsonr method computes both the correlation and p-value for the correlation, roughly showing the probability of an uncorrelated system creating a correlation value of this magnitude.\n",
    "\n",
    "pro's:\n",
    "* faster than other sophisticated methods\n",
    "* can show whether relationship is positive or negative\n",
    "\n",
    "con's:\n",
    "-----> captures only **linear dependency** !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bathrooms (0.52513750541396187, 0.0)\n",
      "sqft_above (0.60556729835607825, 0.0)\n",
      "sqft_basement (0.32381602071198395, 0.0)\n",
      "sqft_lot (0.089660860587100114, 7.9725045103261473e-40)\n",
      "yr_built (0.054011531494792715, 1.929872809374955e-15)\n",
      "yr_renovated (0.12643379344089295, 1.0213478858043326e-77)\n",
      "view (0.39729348829450428, 0.0)\n",
      "waterfront (0.26636943403060209, 0.0)\n",
      "zipcode (-0.053202854298325608, 5.0110505033187622e-15)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "for feature in X.columns:\n",
    "    print feature , pearsonr(X[feature], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bathrooms', 'bathrooms', (1.0, 0.0)),\n",
       " ('bathrooms', 'sqft_above', (0.68534247587615915, 0.0)),\n",
       " ('bathrooms', 'sqft_basement', (0.28377003400466877, 0.0)),\n",
       " ('bathrooms', 'sqft_lot', (0.087739661531261948, 3.3340851719574847e-38)),\n",
       " ('bathrooms', 'yr_built', (0.50601943828525331, 0.0)),\n",
       " ('bathrooms', 'yr_renovated', (0.050738977648059562, 8.4128623727100258e-14)),\n",
       " ('bathrooms', 'view', (0.18773702397664169, 1.1964071644867705e-170)),\n",
       " ('bathrooms', 'waterfront', (0.063743629135636942, 6.5861097041278159e-21)),\n",
       " ('bathrooms', 'zipcode', (-0.20386627357626266, 1.6500992147613623e-201)),\n",
       " ('sqft_above', 'sqft_above', (1.0, 0.0)),\n",
       " ('sqft_above',\n",
       "  'sqft_basement',\n",
       "  (-0.051943306770727428, 2.1539158563766889e-14)),\n",
       " ('sqft_above', 'sqft_lot', (0.18351228086523363, 5.137221092667776e-163)),\n",
       " ('sqft_above', 'yr_built', (0.42389835166374401, 0.0)),\n",
       " ('sqft_above', 'yr_renovated', (0.023284687865467713, 0.0006183571709836459)),\n",
       " ('sqft_above', 'view', (0.16764934410325252, 5.2975803120882837e-136)),\n",
       " ('sqft_above', 'waterfront', (0.072074591672097871, 2.7024901134692345e-26)),\n",
       " ('sqft_above', 'zipcode', (-0.26118997650343961, 0.0)),\n",
       " ('sqft_basement', 'sqft_basement', (1.0, 0.0)),\n",
       " ('sqft_basement', 'sqft_lot', (0.015286201646172797, 0.024622079434632824)),\n",
       " ('sqft_basement', 'yr_built', (-0.1331240989010701, 4.9900800244807298e-86)),\n",
       " ('sqft_basement',\n",
       "  'yr_renovated',\n",
       "  (0.071322901741848688, 8.8041683548076109e-26)),\n",
       " ('sqft_basement', 'view', (0.27694657876758416, 0.0)),\n",
       " ('sqft_basement',\n",
       "  'waterfront',\n",
       "  (0.080587938954292559, 1.7746688093605745e-32)),\n",
       " ('sqft_basement', 'zipcode', (0.07484460782167883, 3.1295400566770031e-28)),\n",
       " ('sqft_lot', 'sqft_lot', (1.0, 0.0)),\n",
       " ('sqft_lot', 'yr_built', (0.053080367025576086, 5.783150287219431e-15)),\n",
       " ('sqft_lot', 'yr_renovated', (0.0076435050135632556, 0.26116126147294733)),\n",
       " ('sqft_lot', 'view', (0.07471010556640087, 3.9010112557489889e-28)),\n",
       " ('sqft_lot', 'waterfront', (0.021603683276613606, 0.0014920655855442117)),\n",
       " ('sqft_lot', 'zipcode', (-0.12957448631737425, 1.4538621798479999e-81)),\n",
       " ('yr_built', 'yr_built', (1.0, 0.0)),\n",
       " ('yr_built', 'yr_renovated', (-0.22487351845188042, 7.3720445189460317e-246)),\n",
       " ('yr_built', 'zipcode', (-0.34686917785526622, 0.0)),\n",
       " ('yr_renovated', 'yr_renovated', (1.0, 0.0)),\n",
       " ('yr_renovated', 'zipcode', (0.06435705728836344, 2.7814994648709019e-21)),\n",
       " ('view', 'yr_built', (-0.053439851392327098, 3.7941409192061689e-15)),\n",
       " ('view', 'yr_renovated', (0.10391728817392427, 5.8102319401628111e-53)),\n",
       " ('view', 'view', (1.0, 0.0)),\n",
       " ('view', 'waterfront', (0.40185735069757117, 0.0)),\n",
       " ('view', 'zipcode', (0.084826916926334581, 8.2024595536344541e-36)),\n",
       " ('waterfront', 'yr_built', (-0.026161085578767569, 0.00011983114979445974)),\n",
       " ('waterfront',\n",
       "  'yr_renovated',\n",
       "  (0.092884836747099755, 1.2630133741539657e-42)),\n",
       " ('waterfront', 'waterfront', (1.0, 0.0)),\n",
       " ('waterfront', 'zipcode', (0.030284727603295605, 8.4666608798982495e-06)),\n",
       " ('zipcode', 'zipcode', (1.0, 0.0))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = X.columns\n",
    "[(i, j, pearsonr(X[i], X[j])) for i in col for j in col if i<=j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn.feature_selection\n",
    "* SelectKBest, SelectPercentile, SelectFpr, SelectFwe, GenericUnivariateSelect \n",
    "\n",
    "* scoring function for regression: f_regression, mutual_info_regression\n",
    "* scoring function for classification: chi2, f_classif, mutual_info_classif\n",
    "\n",
    "http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection\n",
    "\n",
    "example code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8228.9432278 ,  12514.0608974 ,   2531.50632597,    175.14030523,\n",
       "           63.2290479 ,    351.07483789,   4050.45898116,   1650.46303578,\n",
       "           61.34451836])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "SKB = SelectKBest(f_regression, k=4)\n",
    "X_new = SKB.fit_transform(X,y)\n",
    "SKB.scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### F-test:\n",
    "1. The cross correlation between each regressor and the target is computed, that is, ((X[:, i] - mean(X[:, i])) * (y - mean_y)) / (std(X[:, i]) * std(y)).\n",
    "2. It is converted to an F score then to a p-value.\n",
    "\n",
    "-----> captures only **LINEAR DEPENDENCY** ! (http://scikit-learn.org/stable/auto_examples/feature_selection/plot_f_test_vs_mi.html#sphx-glr-auto-examples-feature-selection-plot-f-test-vs-mi-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bathrooms', 0.65757576978979304), ('sqft_above', 1.0), ('sqft_basement', 0.20229295244181325), ('sqft_lot', 0.013995481296054574), ('yr_built', 0.0050526402596444411), ('yr_renovated', 0.028054429394893526), ('view', 0.32367262828331822), ('waterfront', 0.13188868500063505), ('zipcode', 0.0049020472940469236)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "f_test, _ = f_regression(X, y)\n",
    "f_test /= np.max(f_test)\n",
    "print zip(col, f_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MI & MIC :\n",
    "-----> **captures any kind of statistical dependency**\n",
    "\n",
    "But, it can be inconvenient to use directly for feature ranking for two reasons though. Firstly, it is not a metric and not normalized (i.e. doesn’t lie in a fixed range), so the MI values can be incomparable between two datasets. Secondly, it can be inconvenient to compute for continuous variables: in general the variables need to be discretized by binning, but the mutual information score can be quite sensitive to bin selection.\n",
    "\n",
    "**Maximal information coefficient** is a technique developed to address these shortcomings. It searches for optimal binning and turns mutual information score into a metric that lies in range [0;1]. In python, MIC is available in the minepy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bathrooms', 0.48313805417875244), ('sqft_above', 0.62345865146860457), ('sqft_basement', 0.16855055426958279), ('sqft_lot', 0.14501153554990898), ('yr_built', 0.17520338054615969), ('yr_renovated', 0.0032042748241083475), ('view', 0.13446007600176257), ('waterfront', 0.029273394717551429), ('zipcode', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "mi = mutual_info_regression(X, y)\n",
    "mi /= np.max(mi)\n",
    "print zip(col, mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bathrooms 0.184667777851\n",
      "sqft_above 0.217266693397\n",
      "sqft_basement 0.0731383173777\n",
      "sqft_lot 0.0803989424276\n",
      "yr_built 0.0573512220128\n",
      "yr_renovated 0.0389160834264\n",
      "view 0.10493931877\n",
      "waterfront 0.036629801995\n",
      "zipcode 0.384736343043\n"
     ]
    }
   ],
   "source": [
    "from minepy import MINE\n",
    "m = MINE()\n",
    "for c in col:\n",
    "    m.compute_score(X[c], y)\n",
    "    print c,\n",
    "    print m.mic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline\n",
    "\n",
    "Pipeline of transforms with a final estimator.\n",
    "\n",
    "Sequentially apply a list of transforms and a final estimator. **Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit.**\n",
    "\n",
    "The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a ‘__’, as in the example below. A step’s estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting to None.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True,  True, False, False,  True,  True, False,\n",
       "        True, False,  True,  True, False,  True, False,  True,  True,\n",
       "       False, False], dtype=bool)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.datasets import samples_generator\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "# generate some data to play with\n",
    "X, y = samples_generator.make_classification(n_informative=5, n_redundant=0, random_state=42)\n",
    "# ANOVA SVM-C\n",
    "anova_filter = SelectKBest(f_regression, k=5)\n",
    "clf = svm.SVC(kernel='linear')\n",
    "anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])\n",
    "# You can set the parameters using the names issued\n",
    "# For instance, fit using a k of 10 in the SelectKBest\n",
    "# and a parameter 'C' of the svm\n",
    "anova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y)\n",
    "\n",
    "prediction = anova_svm.predict(X)\n",
    "anova_svm.score(X, y)                        \n",
    "\n",
    "# getting the selected features chosen by anova_filter\n",
    "anova_svm.named_steps['anova'].get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor\n",
    "Random Forest for Regression ：\n",
    "* https://www.quora.com/How-does-random-forest-work-for-regression-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many machine learning models have either some inherent internal ranking of features or it is easy to generate the ranking from the structure of the model. This applies to regression models, SVM’s, decision trees, random forests, etc.\n",
    "when all features are on the same scale, the most important features should have the highest coefficients in the model, while features uncorrelated with the output variables should have coefficient values close to zero. This approach can work well even with simple linear regression models when the data is not very noisy (or there is a lot of data compared to the number of features) and the features are (relatively) independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.662, 'LSTAT'), (0.528, 'RM'), (0.407, 'NOX'), (0.329, 'TAX'), (0.302, 'PTRATIO'), (0.287, 'INDUS'), (0.23, 'CRIM'), (0.146, 'ZN'), (0.118, 'RAD'), (0.106, 'DIS'), (0.061, 'B'), (0.017, 'AGE'), (0.004, 'CHAS')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score, ShuffleSplit\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    " \n",
    "#Load boston housing dataset as an example\n",
    "boston = load_boston()\n",
    "X = boston[\"data\"]\n",
    "Y = boston[\"target\"]\n",
    "names = boston[\"feature_names\"]\n",
    " \n",
    "rf = RandomForestRegressor(n_estimators=20, max_depth=4)\n",
    "scores = []\n",
    "for i in range(X.shape[1]):\n",
    "     score = cross_val_score(rf, X[:, i:i+1], Y, scoring=\"r2\",cv=ShuffleSplit(len(X), 3, .3))\n",
    "     scores.append((round(np.mean(score), 3), names[i]))\n",
    "print sorted(scores, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interpreting random forest:\n",
    "http://blog.datadive.net/interpreting-random-forests/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPUTE MISSING DATA\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/missing_values.html#sphx-glr-auto-examples-missing-values-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with the entire dataset = 0.56\n",
      "Score without the samples containing missing values = 0.48\n",
      "Score after imputation of the missing values = 0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apalumbo/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:23: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/apalumbo/anaconda/lib/python2.7/site-packages/numpy/core/numeric.py:190: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  a = empty(shape, dtype, order)\n",
      "/Users/apalumbo/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:27: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "dataset = load_boston()\n",
    "X_full, y_full = dataset.data, dataset.target\n",
    "n_samples = X_full.shape[0]\n",
    "n_features = X_full.shape[1]\n",
    "\n",
    "# Estimate the score on the entire dataset, with no missing values\n",
    "estimator = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "score = cross_val_score(estimator, X_full, y_full).mean()\n",
    "print(\"Score with the entire dataset = %.2f\" % score)\n",
    "\n",
    "# Add missing values in 75% of the lines\n",
    "missing_rate = 0.75\n",
    "n_missing_samples = np.floor(n_samples * missing_rate)\n",
    "missing_samples = np.hstack((np.zeros(n_samples - n_missing_samples,\n",
    "                                      dtype=np.bool),\n",
    "                             np.ones(n_missing_samples,\n",
    "                                     dtype=np.bool)))\n",
    "rng.shuffle(missing_samples)\n",
    "missing_features = rng.randint(0, n_features, n_missing_samples)\n",
    "\n",
    "# Estimate the score without the lines containing missing values\n",
    "X_filtered = X_full[~missing_samples, :]\n",
    "y_filtered = y_full[~missing_samples]\n",
    "estimator = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "score = cross_val_score(estimator, X_filtered, y_filtered).mean()\n",
    "print(\"Score without the samples containing missing values = %.2f\" % score)\n",
    "\n",
    "# Estimate the score after imputation of the missing values\n",
    "X_missing = X_full.copy()\n",
    "X_missing[np.where(missing_samples)[0], missing_features] = 0\n",
    "y_missing = y_full.copy()\n",
    "##################################################################\n",
    "# HERE IS THE IMPUTATION CODE: \n",
    "\n",
    "estimator = Pipeline([(\"imputer\", Imputer(missing_values=0,\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"forest\", RandomForestRegressor(random_state=0,\n",
    "                                                       n_estimators=100))])\n",
    "##################################################################\n",
    "score = cross_val_score(estimator, X_missing, y_missing).mean()\n",
    "print(\"Score after imputation of the missing values = %.2f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultiOutput Regression\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/ensemble/plot_random_forest_regression_multioutput.html#sphx-glr-auto-examples-ensemble-plot-random-forest-regression-multioutput-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "#additional code omitted\n",
    "#y_train has 2 targets, thus dimension of n_features-by-2\n",
    "\n",
    "max_depth = 30\n",
    "#Using MultiOutputRegressor using random forest as \"estimator\"\n",
    "regr_multirf = MultiOutputRegressor(RandomForestRegressor(max_depth=max_depth, random_state=0))\n",
    "regr_multirf.fit(X_train, y_train)\n",
    "\n",
    "#Using Random Forest directly \n",
    "regr_rf = RandomForestRegressor(max_depth=max_depth, random_state=2)\n",
    "regr_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This session is about feature selection when features are uncorrelated.\n",
    "\n",
    "Regressions on scaled features are a way to do feature selection because the coefficient tells the weight each feature has on the prediction."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
